{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Below is to prepare and cleanup the collected data from EJFAT and ERSAP for Digital Twin. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import glob\n","from matplotlib import pyplot as plt\n","import os\n","\n","os.chdir(\"/workspaces/Queue_Simulation_Python/ejfat-data\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# remove outlier data points based on the z-score\n","def remove_outliers(df, columns, threshold=1):\n","    from scipy import stats\n","    import numpy as np\n","\n","    z = np.abs(stats.zscore(df[columns]))\n","    outlier_indices = (z > threshold).any(axis=1)\n","    return df[outlier_indices].index\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dfs_event_rate = []\n","dfs_process_time = []\n","dfs_drop_total_event_rate = []\n","\n","for folder in glob.glob(\"*/\"):\n","    fs = glob.glob(f\"{folder}/*.csv\")\n","    folder = folder.replace(\"dt-vol-\", \"\").replace(\"/\", \"\")\n","    for f in fs:\n","        df = pd.read_csv(f)\n","        if \"currDropTotalEvents\" in f:\n","            df.columns = [\"time\", \"current-drop-total-event-rate\"]\n","            # add a column of folder\n","                # remove \"dt-vol\" in folder name\n","            df[\"folder\"] = folder\n","            dfs_drop_total_event_rate.append(df)\n","\n","        elif \"Process Time\" in f:\n","            df.columns = [\"time\", \"process-time\"]\n","            # the entry is like \"1.22 ms\", and remove the \"ms\" and convert to float\n","            df[\"process-time\"] = df[\"process-time\"].str.replace(\"ms\", \"\").astype(float)\n","            title = \"process-time\"\n","            df[\"folder\"] = folder\n","            dfs_process_time.append(df)\n","\n","        elif \"evRate\" in f:\n","            df.columns = [\"time\", \"event-rate\"]\n","            title = \"event-rate\"\n","            df[\"folder\"] = folder\n","            dfs_event_rate.append(df)\n","\n","dfs_event_rate = pd.concat(dfs_event_rate, ignore_index=True)\n","dfs_process_time = pd.concat(dfs_process_time, ignore_index=True)\n","dfs_drop_total_event_rate = pd.concat(dfs_drop_total_event_rate, ignore_index=True)\n","\n","# remove outliers\n","outliers = remove_outliers(dfs_process_time, [\"process-time\"], 5)\n","print(f\"outliers: {outliers}\")\n","## remove the rows with outliers\n","dfs_process_time = dfs_process_time.drop(outliers)\n","dfs_event_rate = dfs_event_rate.drop(outliers)\n","dfs_drop_total_event_rate = dfs_drop_total_event_rate.drop(outliers)\n","\n","print(f\"type of columns: {dfs_event_rate.dtypes}\")\n","\n","display(dfs_event_rate)\n","display(dfs_process_time)\n","display(dfs_drop_total_event_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot process time\n","plt.figure()\n","for folder, df in dfs_process_time.groupby(\"folder\"):\n","    plt.plot(df[\"time\"], df[\"process-time\"], label=folder)\n","plt.legend()\n","plt.title(\"Process Time\")\n","plt.xlabel(\"time\")\n","plt.ylabel(\"process time (ms)\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","fig, axs = plt.subplots(3, 1, figsize=(10, 15))\n","\n","dataframes = [dfs_event_rate, dfs_process_time, dfs_drop_total_event_rate]\n","for idx, df in enumerate(dataframes):\n","    # get the mean and std of each folder\n","    df_stat = df.groupby(\"folder\").describe()[df.columns[1]].sort_values(\"mean\")\n","\n","    df_mean = df_stat[\"mean\"]\n","    df_std = df_stat[\"std\"]\n","    df_count = df_stat[\"count\"]\n","\n","    # plot mean and std of each folder in the same plot. The dot is the mean, and the error bar is the std centered at the mean\n","    axs[idx].errorbar(df_mean.index, df_mean, yerr=df_std, fmt='o')\n","    # annotate the mean and percentage of the std\n","    for i, (mean, std) in enumerate(zip(df_mean, df_std)):\n","        axs[idx].annotate(f\"{mean:.2f}Â±{std:.2f}\", (i, mean))\n","        \n","        \n","    axs[idx].set_xlabel(\"test\")\n","    # make xlabel vertical\n","    axs[idx].set_xticklabels(df_mean.index, rotation=30)\n","    if \"process-time\" in df.columns[1]:\n","        axs[idx].set_ylabel(\"process time (ms)\")\n","    else:\n","        axs[idx].set_ylabel(df.columns[1]+\" (Hz)\")\n","    axs[idx].set_title(df.columns[1])\n","\n","    # plot the number of data points in each folder as a additional subplot\n","    ax2 = axs[idx].twinx()\n","    ax2.set_ylabel(\"number of data points\")\n","    ax2.bar(df_mean.index, df_count.loc[df_mean.index], alpha=0.2)\n","\n","plt.tight_layout()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Below is for calculating Quantity of interest"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define lamba as dfs_event_rate[\"event-rate\"]\n","ls = dfs_event_rate.groupby(\"folder\").apply(lambda x: x[\"event-rate\"].mean())\n","## add column name lambda\n","ls = ls.reset_index()\n","ls.columns = [\"folder\", \"lambda\"]\n","ls = ls.set_index(\"folder\")\n","# print(f\"ls: {ls}\")\n","\n","\n","# define mu as dfs_process_time[\"process-time\"]\n","mus = dfs_process_time.groupby(\"folder\").apply(lambda x: 1/(x[\"process-time\"].mean()* 1e-3))\n","# print(f\"mus: {mus}\") \n","# add mus as a new column in ls by matching the folder name\n","ls.loc[mus.index, \"mu\"] = mus.values\n","# print(f\"ls: {ls}\")\n","\n","\n","# add a column of server number in ls\n","ls = ls.reset_index()\n","ls[\"server-number\"] = ls[\"folder\"].apply(lambda x: int(x.split(\"-\")[1][0]))\n","ls = ls.set_index(\"folder\")\n","# print(f\"ls: {ls}\")\n","\n","\n","# calculate the utilization\n","ls[\"utilization\"] = ls[\"lambda\"] / ls[\"mu\"] / ls[\"server-number\"]\n","\n","# round ls to 2 decimal places\n","ls = ls.round(2)\n","\n","display(ls)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Get Lq from current total drop events."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df = dfs_drop_total_event_rate.loc[dfs_drop_total_event_rate[\"folder\"] == \"1g-1node\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dfs_drop_total_events = dfs_drop_total_event_rate.copy()\n","dfs_drop_total_events[\"drop-total-events\"] = dfs_drop_total_events.groupby(\"folder\")[\"current-drop-total-event-rate\"].cumsum()\n","dfs_drop_total_events = dfs_drop_total_events.groupby(\"folder\").last()\n","dfs_drop_total_events = dfs_drop_total_events.drop(\"current-drop-total-event-rate\", axis=1)\n","# drop time column\n","dfs_drop_total_events = dfs_drop_total_events.drop(\"time\", axis=1)\n","# \n","display(dfs_drop_total_events)\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":2}
